from datasets import Dataset as HFDataset
from pydantic import BaseModel, Field 
from typing import Optional, Any
from loguru import logger 
import json 
import os 


class Example(BaseModel):
    """Example Schem class used to provide examples for what the user want
    ex
    my_example = Example(question="What is the capital of France?", answer="Paris")
    """
    question: Optional[str] = Field(description="Example question", default=None) 
    answer: Optional[Any] = Field(description="Example answer", default=None)


class Transformation(BaseModel):
    description: str = Field(description="Description of the transformation")
    difficulty_level: float = Field(description="Difficulty level of the transformation from 0 to 1 where 0 means it make the problem easy and 1 means it makes it super hard")

class SubProblem(BaseModel):
    """ Step3 output schema
        we generate list of N sub problems per problem during step3 to make it easier (where N is fixed for now but later 
        should be generated by difficulty Engine)
    """
    question: str = Field(description="Sub question, generated from a main question either eaiser or harder")
    correct_answer: Optional[Any] = Field(description="Correct answer for the question", default=None)
    small_llm_answer: Optional[Any] = Field(description="Answer generated by small LLM", default=None)
    transformations_applid : Optional[list[Transformation]] = Field(description="List of transformations applied to the problem", default=[])
    difficulty_level: Optional[float] = Field(description="Difficulty level of the sub problem from 0 to 1 where 0 is easy and 1 is super hard", default=None)
    is_solvable: bool = Field(description="Whether the sub problem is solvable or not by small llm to be tuned", default=False)

class Problem(BaseModel):
    """Problem Schema"""
    question: str = Field(description="Main question, which we hope to be non solvable by small LLM, but could be solvable by larger LLM")
    correct_answer: Any  = Field(description="Main correct answer for the problem, should be generated by larger LLM or manually", default=None)
    small_llm_answer: Optional[Any] = Field(description="Answer generated by small LLM", default=None)
    sub_problems: list[SubProblem] =  Field(description="List of sub problems generated at step3 to make it easier", default=[])
    difficulty_level: Optional[float] = Field(description="Difficulty level of the problem, from 0 to 1 where 0 is easy and 1 is super hard ", default=0)
    is_solvable: bool = Field(description="Whether the problem is solvable or not by small llm to be tuned", default=False)
    
# VLadder 
class VLADDERItem(Problem):
    """ represents a single vatiant of a problem"""
   
    def to_problem(self):
        return Problem(
            question=self.question,
            correct_answer=self.correct_answer,
            small_llm_answer=self.small_llm_answer,
            difficulty_level=self.difficulty_level,
            sub_problems=self.sub_problems,
            is_solvable=self.is_solvable,
            )

class VLadder(BaseModel):
    """ VLadder dataset schema used in ladder trainined process """
    items: list[VLADDERItem] = []

    def export_to_json(self, path: str=None):
        path = path or "vladder.json"
        with open(path, "w") as f:
            json.dump(self.model_dump(), f, indent=4)
            
    def to_hf_dataset(self) -> HFDataset:
        """ convert vladder dataset to hf dataset """
        # Convert to dict of lists for HF Dataset
        dict_data = {field: [getattr(item, field) for item in self.items]
                     for field in VLADDERItem.model_fields}
        return HFDataset.from_dict(dict_data) # TODO:: retest 

    @staticmethod
    def from_hf_dataset(hf_dataset: HFDataset):
        """ convert hf dataset to vladder dataset """
        # TODO:: verify schema / fields first 
        ds = hf_dataset
        vladder = VLadder()
        # Convert to VLadder format
        vladder.items = [VLADDERItem(
            question=item["prompt"],
            correct_answer=item["completion"],
            difficulty_level=item["difficulty_level"],
            transformations_applied=item["transformations_applied"],
            is_solvable=item["is_solvable"]
        ) for item in ds]
        return vladder
    

    def split(self, ratio: float) -> tuple["VLadder", "VLadder"]:
        """ split vladder dataset into train and test sets """
        train_size = int(len(self.items) * ratio)
        train = VLadder(items=self.items[:train_size])
        test = VLadder(items=self.items[train_size:])
        return train, test
    

    def apply_pattern(self, pattern: str):
        """
            Apply a specific formatting pattern to all completions in the VLadder dataset.
            
            Example:
                If pattern = "Answer: {}"
                Original completion: "The path is balanced."
                Transformed: "Answer: The path is balanced."
            
            Args:
                pattern (str): A string pattern to format completions. Use `{}` as placeholder.
        """
        problems = self.items
        for problem in problems:
            if '{}' in pattern:
                problem.correct_answer = pattern.format(problem.correct_answer)
            else:
                problem.correct_answer = pattern + problem.correct_answer  # fallback: prepend pattern
            
        return self

# This is the main schema flow for finetuing 
# Dataset > VLadder > HFDataset

class Dataset(BaseModel):
    """Dataset Schema"""
    problems: list[Problem]
    description: Optional[str] = None 
    model_intelligence_ratio: Optional[float] = Field(description="Decide how intelligent the model is. The Difficulty Threshold after which LLM cant solve the problem", default=0)
    metadata: Optional[dict] = Field(description="any extra metadata or descriptions you want to store about the dataset", default={})
    @staticmethod
    def from_json(json_path: str) -> "Dataset":
        """ load dataset from json """
        with open(json_path, "r") as f:
            data = json.load(f)
        return Dataset.model_validate(data)
    

    def log(self):
        # logger.warning(self.model_dump())
        logger.debug(f"""
        problems: {self.problems}
        Description: {self.description}
        Model Intelligence Ratio: {self.model_intelligence_ratio}
        Metadata: {self.metadata}
        """)
   
    def export_to_json(self, export_path:str) -> None:
        """ export dataset to json """

        if os.path.exists(export_path):
            logger.warning(f"Dataset already exists at {export_path}. Skipping dataset generation")
            return 
        json_str = json.dumps(self.model_dump(), indent=4)
        with open(export_path, "w") as f:
            f.write(json_str)
        logger.success(f"Dataset exported successfully at {export_path}")
    
    @staticmethod
    def load_hf_dataset(path: str, name: Optional[str] = None, split: Optional[str] = None, **kwargs) -> "Dataset":
        """
        Load a dataset compatible with the Hugging Face `datasets` library.

        Args:
            path (str): Path to the dataset script or JSON/CSV file.
            name (str, optional): Dataset name (if loading from HF hub).
            split (str, optional): Which split to load (e.g., "train", "test").
            **kwargs: Additional arguments passed to `datasets.load_dataset`.

        Returns:
            DatasetDict or Dataset: The loaded dataset.
        """
        # HF Datasets to Dataset
        from datasets import load_dataset as _load_dataset
        ds =  _load_dataset(path, name=name, split=split, **kwargs)
        
        # TODO:: verify if it matches the Dataset schema 
        # TODO:: convert to the vladder format (check if it matches the Dataset schema)
    
    def to_vladder(self) -> "VLadder":
        """ convert dataset to vladder format """
        # self.log()
        vladder = VLadder()
        for problem in self.problems:
            item = VLADDERItem(
                question=problem.question,
                correct_answer=problem.correct_answer,
                small_llm_answer=problem.small_llm_answer,
                difficulty_level=problem.difficulty_level,
                is_solvable=problem.is_solvable,
                sub_problems=problem.sub_problems,
            )
            vladder.items.append(item)

        return vladder


def problems_to_vladder(problems: list[Problem]) -> VLadder:
    vladder = VLadder()
    for problem in problems:
        item1 = VLADDERItem(
            question=problem.question,
            correct_answer=problem.correct_answer,
            small_llm_answer=problem.small_llm_answer,
            difficulty_level=problem.difficulty_level,
            is_solvable=problem.is_solvable
        )
        vladder.items.append(item1)
        for sub_problem in problem.sub_problems:
            if isinstance(sub_problem, tuple) or isinstance(sub_problem, list):
                sub_problem = sub_problem[0]
            item = VLADDERItem(
                question=sub_problem.question,
                correct_answer=sub_problem.correct_answer,
                small_llm_answer=sub_problem.small_llm_answer,
                difficulty_level=problem.difficulty_level,
                transformations_applied=sub_problem.transformations_applid,
                is_solvable=sub_problem.is_solvable
            )
            vladder.items.append(item)
    return vladder