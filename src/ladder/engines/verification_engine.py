from ladder.engines.llm_engine import LLMEngine
from abc import abstractmethod, ABC
from ladder.data_gen.schema import Problem
from typing import Callable, Any
from loguru import logger
import dspy

class AnswerComparator(dspy.Signature):
    """
    You are an answer verification expert specializing in automated evaluation of problem-solving accuracy.

    Your task is to compare the original (reference) answer with a generated answer to determine correctness.
    
    Return 1.0 if the generated answer matches the original correctly, and 0.0 if it is clearly incorrect.
    """

    main_question: str = dspy.InputField(
        prefix="Main Question: ",
        format=str,
        desc="Main question or prompt for which answers are being compared"
    )

    original_answer: str = dspy.InputField(
        prefix="Original Answer: ",
        format=str,
        desc="Reference or correct answer for the problem"
    )

    generated_answer: str = dspy.InputField(
        prefix="Generated Answer: ",
        format=str,
        desc="Answer generated by a model or system to be evaluated"
    )

    result: float = dspy.OutputField(
        prefix="Comparison Result: ",
        format=float,
        desc="0.0 if the generated answer is incorrect, 1.0 if it is clearly correct"
    )

class ProblemSolutionVerifier(dspy.Signature):
    """
    You are a solution verification expert specializing in evaluating the correctness of problem solutions.

    Analyze the provided solution for a given problem and determine if it is correct.
    
    Return 1.0 for a definitely correct solution, and 0.0 if incorrect or invalid.
    """

    problem: str = dspy.InputField(
        prefix="Problem: ",
        format=str,
        desc="Full problem statement that the solution addresses"
    )

    solution: str = dspy.InputField(
        prefix="Solution: ",
        format=str,
        desc="Proposed solution to be verified. You have to compare your solution to the problem with this solution"
    )
    right_solution: str = dspy.OutputField(
        prefix="Right Solution: ",
        format=str,
        desc="Correct solution to the problem"
    )

    result: float = dspy.OutputField(
        prefix="Verification Result: ",
        format=float,
        desc="1.0 if the solution is valid and correct, 0.0 otherwise"
    )


class VerificationEngine(dspy.Module):
    """Problem Verification Engine

    Verifies whether the LLM-generated solution is correct.
    Used during dataset generation and fine-tuning processes.
    """
    # TODO:: this should be small llm to be finetuned not the large one 
    def __init__(self, 
                *, 
                llm_engine:LLMEngine, 
                callbacks: list[Callable]=None):
        super().__init__() 
        self.llm_engine = llm_engine
        self.callbacks = callbacks

        self.problem_solution_verifier = dspy.ChainOfThought(ProblemSolutionVerifier)
        self.answer_comparator = dspy.ChainOfThought(AnswerComparator)

    def verify(self, problem_question: str, given_answer: str) -> float:
        """Automated verification of LLM Solution

        Should return:
        - 1.0 if the solution is correct
        - 0.0 if it is incorrect

        in this base class we will be using the llm_engine to verify the solution , but u can override this for custom verification
        """
        with dspy.context(lm=self.llm_engine.lm):
            res =  self.problem_solution_verifier(problem=problem_question, answer=given_answer)
            logger.warning(f"right_solution: {res.right_solution}")
            return res.result
    
    def get_correct_answer(self, problem_question: str) -> Any:
        """Get the correct answer for the problem question"""
        # if u would like to make sure the generated answer from the LLM is correct u can adjust this 
        # function and add more llm , prompt tuning / reasoning and more advanced answer generation process 

    def compare_answers(self, main_question: str, original_answer: str, generated_answer: str) -> float:
        """ Compare two answers and return a reward value based on the similarity or accuracy of the answers. 
            0: wrong answer
            1: correct answer
        """
        with dspy.context(lm=self.llm_engine.lm):
            res = self.answer_comparator(main_question=main_question, 
                                         original_answer=original_answer, 
                                         generated_answer=generated_answer)
            return res.result
        
    
def verification_reward_factory(engine: VerificationEngine) -> Callable[[list[str], list[str]], list[float]]:
    """
    Factory that produces a reward function from a VerificationEngine instance.
    
    This function wraps the engine.verify method and makes it compatible
    with the GRPO reward function format.
    """
    def reward_func(prompts: list[str], completions: list[str], **kwargs) -> list[float]:
        rewards = []
        for prompt, completion in zip(prompts, completions):
            # Wrap into a Problem schema (adjust as needed)
            problem = Problem(question=prompt, correct_answer=completion) # TODO:: recheck this 
            score = engine.verify(problem)
            rewards.append(score)
        return rewards

    return reward_func